{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04dfd71f-7503-4de9-a190-cc9469afaa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and API\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "# Initialize OpenAI client\n",
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e0015d9-f535-48f8-ba84-2d54dc6ea762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available models\n",
    "MODELS = {\n",
    "    \"GPT-4o Mini\": \"gpt-4o-mini\",\n",
    "    \"GPT-4o\": \"gpt-4o\",\n",
    "    \"GPT-3.5 Turbo\": \"gpt-3.5-turbo\"\n",
    "}\n",
    "\n",
    "# Default model\n",
    "DEFAULT_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# System prompts for different expertise\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"Airline Assistant\": \"You are a helpful assistant for an Airline called FlightAI. Give short, courteous answers, no more than 1 sentence. Always be accurate. If you don't know the answer, say so.\",\n",
    "    \"Technical Expert\": \"You are a technical expert specializing in computer science, programming, and software development. Provide detailed, accurate technical information with code examples when appropriate.\",\n",
    "    \"Language Tutor\": \"You are a language tutor helping students learn new languages. Explain grammar concepts clearly, provide examples, and correct mistakes in a supportive way.\",\n",
    "    \"Custom\": \"\"  # Will be filled by user input\n",
    "}\n",
    "\n",
    "# Default system prompt\n",
    "DEFAULT_SYSTEM_PROMPT = \"Airline Assistant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5283b82a-7b7a-4c1c-8573-f0e8f877968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool functions\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    \"\"\"Get the price of a ticket to the specified destination city.\"\"\"\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")\n",
    "\n",
    "# Function definition for OpenAI\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\",\n",
    "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of tools\n",
    "tools = [{\"type\": \"function\", \"function\": price_function}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "426a2806-6b59-4e35-8ccd-4e0d9726d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    \"\"\"Handle tool calls from the OpenAI API with robust error handling.\"\"\"\n",
    "    try:\n",
    "        # Check if tool_calls exists and is not empty\n",
    "        if not hasattr(message, 'tool_calls') or not message.tool_calls:\n",
    "            print(\"Warning: No tool calls found in message\")\n",
    "            return {\"role\": \"assistant\", \"content\": \"I couldn't process that request.\"}, None\n",
    "        \n",
    "        tool_call = message.tool_calls[0]\n",
    "        \n",
    "        # Check if function and arguments exist\n",
    "        if not hasattr(tool_call, 'function') or not hasattr(tool_call.function, 'arguments'):\n",
    "            print(\"Warning: Tool call missing function or arguments\")\n",
    "            return {\"role\": \"assistant\", \"content\": \"I couldn't process that request.\"}, None\n",
    "        \n",
    "        # Safely parse arguments with error handling\n",
    "        try:\n",
    "            # Check if arguments is empty\n",
    "            if not tool_call.function.arguments or tool_call.function.arguments.strip() == \"\":\n",
    "                print(\"Warning: Empty arguments in tool call\")\n",
    "                arguments = {}\n",
    "            else:\n",
    "                arguments = json.loads(tool_call.function.arguments)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Failed to parse tool call arguments: {e}\")\n",
    "            print(f\"Arguments received: '{tool_call.function.arguments}'\")\n",
    "            arguments = {}\n",
    "        \n",
    "        # Get city with fallback\n",
    "        city = arguments.get('destination_city', \"unknown\")\n",
    "        price = get_ticket_price(city)\n",
    "        \n",
    "        # Create response with proper tool_call_id\n",
    "        response = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps({\"destination_city\": city, \"price\": price}),\n",
    "            \"tool_call_id\": tool_call.id if hasattr(tool_call, 'id') else \"unknown_id\"\n",
    "        }\n",
    "        \n",
    "        return response, city\n",
    "    except Exception as e:\n",
    "        # Catch-all for any other unexpected errors\n",
    "        print(f\"Error in handle_tool_call: {str(e)}\")\n",
    "        return {\"role\": \"assistant\", \"content\": \"I encountered an error while processing your request.\"}, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46ecfa08-b55f-4017-9be1-34ab79c8cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_streaming(message, history, system_prompt_key, custom_system_prompt, model_name):\n",
    "    \"\"\"Chat function with streaming support.\"\"\"\n",
    "    # Determine which system prompt to use\n",
    "    if system_prompt_key == \"Custom\":\n",
    "        system_message = custom_system_prompt\n",
    "    else:\n",
    "        system_message = SYSTEM_PROMPTS[system_prompt_key]\n",
    "    \n",
    "    # Convert history to the format expected by OpenAI\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    # Process history - ensure it's in the right format for OpenAI\n",
    "    # Handle both tuple format and dictionary format for backward compatibility\n",
    "    if history:\n",
    "        for msg in history:\n",
    "            if isinstance(msg, dict):\n",
    "                # If it's already a dict with role and content, use it directly\n",
    "                if \"role\" in msg and \"content\" in msg:\n",
    "                    messages.append(msg)\n",
    "            elif isinstance(msg, (list, tuple)) and len(msg) == 2:\n",
    "                # If it's a tuple/list of (user_msg, assistant_msg), convert to dicts\n",
    "                user_msg, assistant_msg = msg\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    \n",
    "    # Add the current message\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    # Get the model from the selected name\n",
    "    model = MODELS[model_name]\n",
    "    \n",
    "    # Create a streaming response\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Variables to track the response\n",
    "    collected_messages = []\n",
    "    finish_reason = None\n",
    "    message_obj = None\n",
    "    \n",
    "    # Process the streaming response\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            collected_messages.append(chunk.choices[0].delta.content)\n",
    "            partial_message = \"\".join(collected_messages)\n",
    "            yield partial_message\n",
    "        \n",
    "        # Check if we have a tool call\n",
    "        if chunk.choices[0].finish_reason:\n",
    "            finish_reason = chunk.choices[0].finish_reason\n",
    "        \n",
    "        # Store the message object if it has tool calls\n",
    "        if hasattr(chunk.choices[0].delta, 'tool_calls') and chunk.choices[0].delta.tool_calls:\n",
    "            if message_obj is None:\n",
    "                message_obj = chunk.choices[0].delta\n",
    "            else:\n",
    "                # Append tool call information\n",
    "                if not hasattr(message_obj, 'tool_calls'):\n",
    "                    message_obj.tool_calls = []\n",
    "                message_obj.tool_calls.extend(chunk.choices[0].delta.tool_calls)\n",
    "    \n",
    "    # If we need to handle a tool call\n",
    "    if finish_reason == \"tool_calls\" and message_obj and hasattr(message_obj, 'tool_calls'):\n",
    "        # Handle the tool call\n",
    "        tool_response, city = handle_tool_call(message_obj)\n",
    "        \n",
    "        # Add the tool call and response to messages\n",
    "        messages.append({\"role\": \"assistant\", \"content\": None, \"tool_calls\": message_obj.tool_calls})\n",
    "        messages.append(tool_response)\n",
    "        \n",
    "        # Get a new response from the model\n",
    "        second_response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Reset collected messages\n",
    "        collected_messages = []\n",
    "        \n",
    "        # Process the second streaming response\n",
    "        for chunk in second_response:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                collected_messages.append(chunk.choices[0].delta.content)\n",
    "                partial_message = \"\".join(collected_messages)\n",
    "                yield partial_message\n",
    "    \n",
    "    # Return the final message\n",
    "    final_message = \"\".join(collected_messages)\n",
    "    return final_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "911603be-41ae-4ae2-861a-b27f6a071bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ui():\n",
    "    \"\"\"Create the Gradio UI for the application.\"\"\"\n",
    "    with gr.Blocks(title=\"Technical Question/Answerer\") as demo:\n",
    "        gr.Markdown(\"# Technical Question/Answerer Prototype\")\n",
    "        gr.Markdown(\"Ask questions and get answers with streaming responses, model switching, and tool integration.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                # System prompt selection\n",
    "                system_prompt_dropdown = gr.Dropdown(\n",
    "                    choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                    value=DEFAULT_SYSTEM_PROMPT,\n",
    "                    label=\"System Prompt (Expertise)\"\n",
    "                )\n",
    "                \n",
    "                # Custom system prompt input\n",
    "                custom_system_prompt = gr.Textbox(\n",
    "                    lines=3,\n",
    "                    placeholder=\"Enter your custom system prompt here...\",\n",
    "                    label=\"Custom System Prompt\",\n",
    "                    visible=False\n",
    "                )\n",
    "                \n",
    "                # Model selection\n",
    "                model_dropdown = gr.Dropdown(\n",
    "                    choices=list(MODELS.keys()),\n",
    "                    value=\"GPT-4o Mini\",\n",
    "                    label=\"Model\"\n",
    "                )\n",
    "            \n",
    "            with gr.Column(scale=2):\n",
    "                # Chat interface - explicitly set type to 'messages' to avoid deprecation warning\n",
    "                chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "                msg = gr.Textbox(placeholder=\"Type your message here...\", label=\"Message\")\n",
    "        \n",
    "        # Event handlers\n",
    "        def update_custom_prompt_visibility(prompt_key):\n",
    "            return {\"visible\": prompt_key == \"Custom\"}\n",
    "        \n",
    "        system_prompt_dropdown.change(\n",
    "            fn=update_custom_prompt_visibility,\n",
    "            inputs=system_prompt_dropdown,\n",
    "            outputs=custom_system_prompt\n",
    "        )\n",
    "        \n",
    "        # Handle text input - ensure we're updating the chatbot correctly with message dictionaries\n",
    "        def process_chat(message, history, system_prompt_key, custom_system_prompt, model_name):\n",
    "            # Initialize history if None\n",
    "            history = history or []\n",
    "            \n",
    "            # Get the streaming response\n",
    "            response_generator = chat_with_streaming(message, history, system_prompt_key, custom_system_prompt, model_name)\n",
    "            \n",
    "            # Add user message as a dictionary with role and content\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            # Add initial empty assistant message\n",
    "            history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "            \n",
    "            # Update the assistant's message as the response streams in\n",
    "            for partial_response in response_generator:\n",
    "                history[-1][\"content\"] = partial_response\n",
    "                yield history\n",
    "        \n",
    "        msg.submit(\n",
    "            fn=process_chat,\n",
    "            inputs=[msg, chatbot, system_prompt_dropdown, custom_system_prompt, model_dropdown],\n",
    "            outputs=chatbot,\n",
    "            queue=False\n",
    "        ).then(\n",
    "            lambda: \"\",\n",
    "            None,\n",
    "            msg\n",
    "        )\n",
    "        \n",
    "    return demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9827b7fe-4dd1-40ce-b113-94f767595c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\routes.py\", line 1191, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2146, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1676, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 729, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 723, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 706, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 867, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\AppData\\Local\\Temp\\ipykernel_4628\\1365527677.py\", line 60, in process_chat\n",
      "    for partial_response in response_generator:\n",
      "  File \"C:\\Users\\simo_\\AppData\\Local\\Temp\\ipykernel_4628\\2379167129.py\", line 76, in chat_with_streaming\n",
      "    second_response = openai.chat.completions.create(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py\", line 1239, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\simo_\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py\", line 1034, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"Missing required parameter: 'messages[8].tool_calls[1].id'.\", 'type': 'invalid_request_error', 'param': 'messages[8].tool_calls[1].id', 'code': 'missing_required_parameter'}}\n"
     ]
    }
   ],
   "source": [
    "# Create and launch the UI\n",
    "# IMPORTANT: Do NOT use share=True to avoid antivirus conflicts\n",
    "demo = create_ui()\n",
    "demo.launch()  # Removed share=True to avoid antivirus conflicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb65ee2a-5a22-47b8-908d-7d00a1ea1644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ticket price tool:\n",
      "Tool get_ticket_price called for London\n",
      "Price for London: $799\n",
      "Tool get_ticket_price called for Paris\n",
      "Price for Paris: $899\n",
      "Tool get_ticket_price called for Tokyo\n",
      "Price for Tokyo: $1400\n",
      "Tool get_ticket_price called for Berlin\n",
      "Price for Berlin: $499\n",
      "Tool get_ticket_price called for Unknown City\n",
      "Price for Unknown City: Unknown\n",
      "Warning: Empty arguments in tool call\n",
      "Tool get_ticket_price called for unknown\n"
     ]
    }
   ],
   "source": [
    "# Test the ticket price tool\n",
    "print(\"Testing ticket price tool:\")\n",
    "cities = [\"London\", \"Paris\", \"Tokyo\", \"Berlin\", \"Unknown City\"]\n",
    "for city in cities:\n",
    "    price = get_ticket_price(city)\n",
    "    print(f\"Price for {city}: {price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0aacdd-f973-46a1-9bf6-b27c8db59212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
